## Data Analytics

> [!info] High performance data analysis

### Kinesis Data Streams

> tldr; High performance streaming service,
> allows large scale of ingestion into aws and consumption by other compute resources

Highly Available by default

Producers send data into Kinesis stream

Public Service

**Real time**

**Streams store 24 hours worth of data**

Producer --> Kinesis Stream <-- Consumer

Sharded design

* Scales by increasing number of shards
* Each shard gives 1MB ingestion, 2MB consumption
* 24 hour window, up to 365 days

#### SQS vs Kinesis

==Exam: Is the question about ingestion of data? -> Kinesis==

Or is it about worker pools decoupling? Or async communications? -> SQS

SQS queues are ususually used for decoupling and asynch communications. Messages on queues are not persisted - no window.

Kinesis: Designed for huge scale ingestion of data, lots of things sending data into the stream, multiple consumers, rolling window.

### Kinesis Data Firehose

> tldr; stream based delivery service, sends the data to something else

> tldr; data delivery service

Data Firehose offers a way to persist Kinesis data

Data into S3 or Data Lakes or other data analytics services

Automatic scaling, serverless, resilient

**Near Real Time (~60 seconds)**

Offers transformation of data on-the-fly in a Lambda

Billing is based on Data Volume through the service

Destinations DFH can deliver to:

* Http endpoints
* Splunk
* Redshift
* ElasticSearch
* S3

Firehose buffers input: ~60 seconds until the 1MB data fills up

Firehose data can be transformed via Lambda "Blueprints"

Redshift destination involves an intermediate S3 bucket, but this is handled by firehose product

### Kinesis Data Analytics

> tldr; real time data processing transformed by sql query

Ingests from Kinesis Data Streams or Firehose

Real time service (RT)

Targets -> Firehose (NRT), Lambda (RT), Data Streams (RT)

Reference data stored in S3 bucket (static data used to enrich streaming input)

Think of this like a SQL processing layer inbetween source and target

Reference table is combined with source data + SQL query to generate new output

Source Stream + Reference Data -> Data Analytics SQL -> Destination

Quite an expensive product!

Use cases: Streaming data needing real time SQL, Elections, E-sports, leaderboards, security & response teams

### Map Reduce

> tldr; Framework that allows parallel processing of huge datasets

Phases:
* Two phases: Map and Reduce
* Optional: Combine & Partition
* Data separated into Splits, each is assigned to a Mapper

Example:
* Input data: <massive text tile with all books ever, count each word>
* Split (each piece is a book)
* Each split is assigned to a mapper
* Shuffle (number of occurances grouped together)
* Reduce: the(50), best(20)
* Final output, collect this and order

HFDS = Hadoop File System, traditionally stored across multiple data nodes, HA

### EMR (Elastic Map Reduce)

> tldr; managed implementation of Apache Hadoop

Spark, HBase, Presto, Flink, Hive, Pig

Hadoop clusters = high maintainence

EMR allows for transient clusters, or long term clusters

Only runs in One AZ

Uses EC2 - can use Spot, Instance Fleet, Reserved,  On-demand

Loads data from S3 -> Output data stored in S3 too

Cluster:

* **Master Node**
  * Recent EMR allows for 3x Master node for HA
* Acts as Name node
* Then we have **Core nodes**
  * These track completion of workloads
  * These run HDFS
  * (disruption is bad don't run on spot instance)
* **Task nodes**
  * Ideal for spot instances
  * Doesn't matter if disrupted as much
* EMRFS
  * S3 based backing data store that provides AZ resilience
  * Not as high performance as HDFS which is on instance store volumes

==Exam: On demand for Master, Core Nodes, Spot for Task nodes==

### Redshift

> tldr; Petabyte-scale Data Warehouse based on Postgres

OLAP database - analytics

"Cube"

Redshift Spectrum

* Use your Redshift cluster to query S3 directly
* Tend to look for Athena for this rather than redshift

Federated query:

* Query other DBs via Redshift

JDBC/ODBC standard

Provisioned product that uses servers (not serverless)

**Runs in One AZ - not fully HA by design**

Cluster:

* Leader Node - query input, planning, aggregation
  * Applications / clients connect to leader node

* Compute Node - performing queries of data
  * Also called slices

Supports: VPC Security, IAM, KMS, CW Monitoring

Enhanced VPC routing feature

* By default redshift will use public routes for traffic for external services, e.g. s3
* Enabling enhanced routing means traffic behaves like other stuff in VPC
* We need this for better control

Only runs from 1 AZ but:

* Additional node
* Data backed up to S3 every 8 hours or 5GB of data
* Can also do manual snapshots onto S3
* Can also set up automated snapshots copy to another region for DR
* Need to copy the encryption key too via a special process

Can migrate data in from:
* S3
* DMS
* Firehose

### AWS Batch

> tldr; Big jobs that can run without end user interaction, e.g. simulations

> tldr; Lambda on roids for running huge jobs

You upload data to S3

This triggers an event

Result stored on S3

AWS batch comes in when jobs are big and we need more than the 15 mins of Lambda

Basic architecture:

* Job - this is what we want done
  * Script, executable or docker container
  * Job definition:
    * Metadata for a job, permissions, resource config, mount points
  * Job Queue
  * Compute Environment
    * Managed or unmanaged compute
    * AWS batch handles scaling etc on your behalf

Can have high priority and low priority queues

Sources:

* S3 -> Eventbridge -> Batch
* Lambda
* Step Functions State Machine -> API calls

What batch does:

* Spin up environment
* Complete job
* Keep track of state in DynamoDB
* Store output data on S3
* AWS Batch Event Stream to trigger further processing
* Or step functions state machine next step

Limitations of Lambda

* 15 min execution
* Limited disk space
  * EFS fixes this but has security implications
* Fully serverless but limited runtime selection

Compared to Batch

* Batch is not serverless
* It uses docker
* No time limit or resource limite

Compute options:

* Managed
  * On-demand or Spot Instances, you choose size
  * You set max spot price
  * Compute runs in a VPC, requires VPC gateways to provide resources needed
* Unmanaged
  * You already have your environment spun up and want to point Batch at it
* Most people use Managed, it has low overhead

### Amazon QuickSight

> tldr; Business Intelligence service with per session pricing model

> tldr; very nice product in real world usage

Visualisations, ad-hoc data analysis

Discover and Integration with AWS Data sources

And works with external data sources

==Exam: Hear Dashboards / Visualisation think QuickSight==

Supports:

* Athena, Aurora, Redshift, Redshift Spectrum
* S3, AWS IOT
* Jira, Github, Twitter, SalesForce
* SQL Server, MySQL, PostgresSQL
* Apache Spark, Snowflake, Presto, Teradata