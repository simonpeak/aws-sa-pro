## Networking & Hybrid

> [!info] Advanced Networking between AWS and on-premise
### Advanced VPC Routing

A subnet can only be associated with one route table
- Either main RT (created by default)
- or custom

Not possible to have subnet without route association
- Associate with IGW or VPW (Virtual Private Gateway)
- IPv4 and IpV6 handled separately within RT

Routes have **destinations** (CIDR range) and **targets** (device) and **priority**

Route tables
- Applied per subnet, you could route to different place in subnet A than subnet B
- Static routes (added manually by us)
- Propagated routes (added by VGW)
    - Learned from DX, Site to site VPN
    - Either via BGP or statically defined within VPN config
##### Order

- Routes applied in order, more specific takes priority
    - Based on prefix length
    - Longer prefix = higher priority
        - e.g. /24 takes priority over /16
- Static routes take priority over propagate if all else equal
    - Things you specifically add take preference over learned things
- If all else equal we use this order for propagated routes
    1. DX
    2. VPN static
    3. VPN BGP
    4. AS_PATH
##### Ingress Routing

- Gateway route tables
- Assign a route table to gateway objects
    - Internet Gateway        
    - Virtual Private Gateway
- Enables scenario like sending traffic through to security inspection device instead of actual destination
- Traffic from host -> Sec appliance -> IGW -> Internet -> IGW -> Sec Appliance -> Host

### Public vs Private Services

> tldr; Three important network zones, Public Internet, AWS Public, AWS Private

Public Service: Public Endpoints, e.g. S3

A VPC is a Private Zone

Public Internet is different to AWS Public

Permissions and Networking both need to be factored in when connecting to public services

==Exam: Data going from VPC through IGW to AWS Public Services doesn't go through Public Internet Zone==

### Local Zones

> tldr; latency sensitive applications can be affected by distances from business to AZ

Use when you need THE HIGHEST performance / lowest latency

Direct connect to local zones is usually supported

The set-up
- Parent Region: us-west-2
- Local Zone: us-west-2-**las-1**

Different services support local zones in different ways
- e.g. VPC can be extended into local zone by adding a subnet there

Local zone subnets have REALLY low latency
- Think about local zones as one additional AZ near your location
- Imagine taking one AZ and spinning it up really close to your building
- But... not all products support them
	- Opt in
	- Some come with caveats

### Global Accelerator

> tldr; a way of moving content closer to users at the network level using anycast ips

==Exam: When should you  use CloudFront vs Global Accelerator?==

Unicast vs Anycast IP address
- Unicast - one destination
- Anycast - routed to the IP at the closest location

Public internet only used to get to the Global Accelerator location, then goes through AWS global network

How do we set this up?
- We create a global accelerator, we get Anycast ip address to use

GA vs CloudFront - they are similar but different use cases
- GA is a network product can be used for non-https, CloudFront: http/s
- GA doesn't understand protocols
- If you need to cache web, it'll be CloudFront
- If you need global tcp/udp optimization: Global Accelerator

### DHCP

> tldr; How resources in our VPC get an IP address

MAC address -> broadcast to find DHCP server -> receive allocated IP address, subnet mask, default gateway etc

Also receive DNS servers
- Typically Amazon provided ones (Route53 resolver)
- Aa custom domain can be configured

NetBIOS stuff if protocol if being used

#### DHCP Option Sets

Once created they can't be changed (immutable)
- 0 or 1 per VPC
- Only things you can change 
- Which DNS servers to use?
- NTP server for time sync

### Network Access Control Lists (NACL)

> tldr; traditional firewall available within AWS VPCs, only affects data crossing subnet boundaries

Every subnet has a NACL association

Connections within a subnet do not pass through NACLs

Stateless - they don't know if traffic is request or response
- Means you need to remember to create inbound and outbound rules

NACLs offer BOTH explicit Allows and explicit Denys
- There are always two sets of rules, inbound and outbound
- Rules are evaluated in priority order
- Rule numbers are independent on inbound and outbound lists
- Processing stops immediately after a rule is matched
- Catch-all only applies if nothing else matches

Default NACL has allow all and deny all -- net result traffic is allowed
- Beginner friendly
- AWS prefer security groups

NACLS are not aware of logical resources, only CIDR range and port based on subnets

NACLs often used to DENY traffic (e.g. bad network actors), combine with Security Groups to allow traffic

One NACL can be associated with multiple subnets

### Security Groups (SGs)

> tldr; stateful version of a firewall, applies to ENI (*not* instances or subnets)

==Exam: when a SG is hooked up to an 'instance' it's attached to primary network interface (ENI) of the instance.==

Conceptually: An SG surrounds an ENI

Operate above NACLs on OSI layer 7 stack
- More features: IP/CIDR and logical resources
  - Can reference other security groups, e.g. Web_Server_SG
  - Scales well, if another instance is added to referenced SG, it's handled automatically
    - E.g. when autoscaling
  - Can also be self referencing
    - Say you have a group of EC2 instances and you need to allow full access to each other, but restrict traffic from outside

Stateful: No need to worry about other the side of rules, or ephemeral ports

Major limitation: No explicit deny
- We have implicit deny (anything that is not in the included list)
- This matters when: say you want to block the IP of one bad actor, you can't do it with an SG -- use NACLs
- e.g. you allow 0.0.0.0/0 on port 443, you can't deny 1.2.3.4 IP address for bad actor Barry

#### Site-to-Site VPN

> tldr; a cheaper faster way of connecting from on-prem to AWS than DX

What's involved?

- VGW (Virtual Private Gateway) - goes in a VPC and becomes a target on route tables
  - VGW object is HA like IGW
- CGW (Customer Gateway)
  - Two parts to this
    - Logical config object in AWS
    - or the Physical device, e.g. customer router where VPN terminates
- VPN connection between VPW and CGW
  - Static vs Dynamic
    - Static means static routes need to be configured with IP addressing info on both sides
    - Dynamic uses BGP, network info is exchanged and adjusts routing on the fly, links dynamically adjusted
      - Route propagation if enabled allows routes to be learned
- 2 public space endpoints are created in different AZs

Can be fully highly available
- But has to be designed right!
  - Customer router can be a single point of failure
    - Solution: add 2nd router in another site if possible, create additional vpn connection

Quick to provision
- Less than 1hr
- In contrast to long provisioning time of DX

Limitations
- ==Exam fact: Speed cap of 1.25Gb/s at AWS side==
- VGW limitation is also 1.25Gb/s
- Encryption overhead
- Latency considerations, we're going over the public internet
- BGP support needed for Dynamic VPNs

Costs
- Hourly cost
- Per GB cost for outgoing data
- We also have to watch out for data caps on your on-prem internet connection

Use case: Use VPN to start quickly, add DX, keep VPN for redundancy or replace with 2nd DX

Use case: VPN over top of DX to add encryption... or use MACsec to keep things simpler (see later on)

### Accelerated Site-to-site VPN

> tldr; uses Global Accelerator to speed up site-to-site VPN, but requires Transit Gateway

Alternative to running site-to-site VPN over DX

TGW can be the endpoint for VPNs to simplify config for multiple VPCs
- Acceleration can only be used when using TGW - VGWs don't support accelerated VPN

==Exam: TGW is preferred way to do VPNs, it's getting better features over time than VGW==

In accelerated mode, we connect to AWS Global Network via Anycast IPs
- Public internet only used to get to closest edge location
- Result: Better quality connection (lower latency, higher throughput)

Cost:
- Fixed accelerator fee
- Transfer fee

Outside IP - external IP publicly accessible for each end of VPN
- The tunnel setup itself
Inside IP - once tunnel is established - actual data and BGP traffic
- data flowing in the tunnel
#### Client VPN

> tldr; a secure way to access VPC resources from your client machine, you connect and it's like the AWS VPC is your local network

Managed implementation of OpenVPN

Pretty much any OS of consumer grade router can support it

Associated to 1 VPC and 1+ Networks
+ Billed on subnet associations
+ And per hour for each client connection
+ "Client VPN ENI"
+ Only 1 subnet per AZ

Connection logs stored in CloudWatch

Client behaves as if it's in the VPC
+ Would need a IGW for internet access etc

By default client route table is replaced (default path)
+ All connections go via the VPN

But another option is Split Tunnel VPN (optimal path)
- Route tables on client are not replaced
- Clients to AWS routes are added on

Steps to getting this working:
1) Create client VPN Endpoint
2) Associate with a subnet in your VPC
3) Add authorization rule allowing access to VPC CIDR
4) Run AWS Client VPN software
5) Connect in using credentials (e.g. SimpleAD user)
### Transit Gateway (TGW)

> tldr; simplifies connecting stuff together, results in a less complex network architecture overall

HA and scalable

Supports transitive routing

Connects to other things with *attachments*
- VPC
- VPN attachments
- Direct Connect Gateway (DX)
  - 1 DX gateway can be attached to 3 TGWs
- And peering...
  - To other TGWs

Fixes scaling VPC problems
- VPC doesn't support transitive routing
- Customer Gateway (CGW) requires connection from every peered VPC
  - Need two of these for HA!

TGW can connect to CGW
- Would just need two VPN tunnels

VPC Attachment
- Configure with subnet in each AZ where service is required

TGW can be peered with other TGWs
- via peering attachment
- Traffic goes through AWS network rather than public internet

AWS RAM can share TGWs between accounts

TGW has one Route Table

Each attachment adds routes it then knows about
- except peering attachments
- Need static routes to allow stuff in AZ1 to talk to AZ2
- Potential future enhancement to enable this
  - Use unique ASNs

Resolution of Public DNS to Private IP doesn't work over TGW Peering due to cross region

Data is encrypted

Attachment can only be associated with 1RT

RTs can be associated with many attachments

Association means a route table is associated with an attachment, used when data is exiting an attachment

Propagation refers to when a route table is automatically populated by known routes from attachments
### Direct Connect (DX)

#### Overview

> tldr; Dedicated physical connection to AWS

Your business -> DX location -> AWS region

Cost
- Hourly cost (speed, location)
- Outbound data cost (inbound is free)

How do you set it up?
- Request DX port allocation from AWS
- Arrange for your business to connect to the DX location (~weeks/months)
- Note: No resilience - this is just one connection

Benefits: 
- Low & consistent latency
- Speed, 1, 10, 100Gbps
- Compared to IPSEC vpn no encryption overhead

DX is for AWS Private Services (VPCs) and AWS Public Services - No internet access!

DX location building is NOT owned by AWS
- Might be inside a shared DC
- AWS will have a cage in there with DX endpoints
- You might also have a customer cage in the DC
  - Or shared with a communications partner
- "Cross connect" term for the physical port between your cage and aws dx cage
  - The AWS DX cage is connected to the AWS region via multiple network connections

#### VIFs

We configure VIFs (Virtual Inferfaces) over this end-to-end DX connection

A virtual interface (VIF) is necessary to access AWS services, and is either public or private. 
- Transit VIF - used to access one or more DXs associated with a Transit Gatewa
- Public VIF - provide access to AWS public services like S3
- Private VIFs provide access to your VPC through Virtual Private Gateway (VPG)

#### DX Physical Connection

A DX doesn't actually provide a connection of any kind
- Just a port allocation at a DX location
- AWS direct offers 1, 10, 100Gbps

DX can be ordered through partner, but you might get less speed/functionality

Single-mode fibre only - NO copper
- Auto negotiation disabled
- Manually set port speed and duplex manually set
- BGP and BGP MD5 authentication
- Optional... MACsec, Bidirectional Forwarding Detection (BFD)

#### MACsec

> tldr; improves long standing problem with DX -- no encryption

MACsec is a standard that extends ethernet

Layer 2 adjacency
- Works from one hop to the next
- Data integrity - additional fields to detect modifications
- Data origin authenticity - both parties can see the frames were sent by a trusted peer

Does not replace IPSEC over Public VIF
- Because it's not end-to-end encryption
- Also allows you to configure connection to VPC outside of local DX region

Each MACsec pair creates a secure channel
- Unidirectional (1 in 1 out)
- SCI secure channel identifier

Adds MACsec tag and Integrity Check tags to Ethernet

Why do we need MACsec?
- Without it, we have unencrypted data being transferred through non-aws owned DC
- e.g. cross connect between customer cage and aws cage

MACsec set up on AWS cage and Customer cage in DX router

#### DX Connection Process

> tldr; how the heck to we connect our equipment to aws?

DX Location

- AWS don't own it
- Neither do we
- Both parties typically rent space from datacenter provider

So we need DC operator to connect AWS cage to customer cage

Authorization needed from both parties

- Letter of Authorization Customer Facility Access (LOA-CFA)

The steps

1) Bob lodges Connection Request with AWS
2) AWS allocate a port on their DX router
3) Bob downloads LOA-CFA form (form can only be downloaded once port is allocated)
4) Bob provides completed form 
5) DC provider begins 'cross connect' provisioning
6) DC staff provide cross-connect between AWS DX router and Bob's customer router

#### DX Virtual Interfaces (VIFs)

> tldr; Interface to multiple networks over a single connection using VLANs + BGP

We have a physical connection, with Layer 2 datalink established 'layer 2 adjacency'

We have multiple networks are running over this one link

Each is differentiated by:

1) VLAN to isolate traffic
2) BGP to exchange prefixes so each side knows the other side's networks
3) MD5 authentication to ensure only valid information is allowed

One example, we need a network connection for Public, Private and Transit networks into AWS over one DX connection, VIFs make this possible

One *dedicated* DX connection can have 50 public/private VIFs + 1 Transit VIF

*Hosted* DX can only have 1 VIF!

#### Private VIFs

> tldr; you have a working DX connection and need to access private ip addresses in your vpc

Associated with a Virtual Private Gateway
- Must be in same region as VPC
- 1 Private VIF -> 1 VGW -> 1VPC

No encryption by default

Jumbo frames are supported

Using VGW enables route propagation -> Learned routes are added to route tables automatically

To create:
- Need account for ownership (supports multi account), VLAN id , BGP ASN
- 100 prefix limit for corp prefixes

==Exam: Doesn't scale to multi region, Private VIFs can only connect to VPCs in same region as DX location==

==Exam: You need a DX in the other region too! Scales badly==

==Exam: DX Gateway is an alternative for this requirement==

#### Public VIFs

> tldr; you have a working DX connection and need to access public services like s3 or elastic IPs

- AWS advertise routes to other regions so we can get traffic to them (except China)

Support bi-directional BGP communities
- Not transitive - routes don't leave AWS

==Exam: Public VIFs don't have the regional limitation that private VIFs have==

#### Use case: Direct Connect + Public VIF + VPN

> tldr; VPN over DX gives low latency encrypted tunnel - high performance & security

Remember: DX not encrypted

Use a public VIF, private VIF would only give private IP access

End to end encryption - better than MACsec in this regard

IPSEC VPNs are better supported by routers than MACsec

However: VPN has more cryptographic overhead vs MACsec - limits speeds

Common form of resilience used by customers:

- DX with Public VIF + VPN (primary, fast speeds)
- Plus Normal internet connection with IPSEC VPN into AWS (backup, quick to provision)

Using VPN allows connections to VPCs in remote regions over global AWS network (not supported by private VIFs)

#### Direct Connect Gateways

> tldr; Global device, associate it with a private VIF instead of VGW, an intermediate layer

Private VIFs can't handle routing outside the local region, DX gateway scales better with multiple VPCs

No cost for DX Gateway object - but you pay for traffic

==Exam: No routing between each VPC connected via DX Gateway==

==Exam: Two sites connected to a VPC via DX Gateway also can't talk to each other==

==Exam: Adding a TGW solves this==

Example:
- Customer side -> DX Gateway -> VGW -> VPC1
- Customer side -> DX Gateway -> VGW -> VPC2
- No VPC1 -> VPC2 traffic

Each DX connection can support 50 DX GWs

10 VPCs per DX Gateway

#### Transit VIFs and TGW

> tldr; a way of associating dx gateways with transit gateways for better routing

Note: Transit Gateways are regional devices but they can be peered with TGWs in other regions.

1 Transit VIF per DX connection

DX -> DXGW -> Up to 3 TGWs

==Exam: A DX gateway can only be used for either Transit VIFs or Private VIFs but not both==

==Exam: DXGW doesn't route between attachments, you still need peered TGWs for this==

Various limits:
- Max 20 DXGW per TGW
- Each TGW supports 50 peering attachments
- Each TGW support 5000 attachments
- Each TGW can be peered with up to 5000 other TGWS each of which can have up to 50 peering attachments!

#### DX Resilience

> tldr; A single DX connection is not resilient at all - 7 single points of failure!

Resilience can be improved by:

- Requesting a second DX port at DX location
  - But watch out for hidden SPOFs
- Add Second DX location + Second customer premises
  - We can lose a DX location or one customer premises
- Extreme resilience:
  - 2 DX ports
  - 2 DX locations
  - 2 Customer sites

Can use site-to-site VPN as a backup too

#### DX Link Aggregation Groups (LAG)

> tldr; Combine 2-4 direct connects into one logical connection

==Exam: Main benefit is speed==

e.g. 2x 100Gig connections

e.g. 4x 10Gig connections

LAGs need to use the same speed of connection

LAGs need to go into the same DX location

Best practice to allocate DX when creating lags, to avoid not having enough ports later on
### Route 53

> tldr; AWS managed domain service

- Register Domains
  - Money -> 4x Name Servers -> Zone file on those NS -> Shares with TLD registry
  - Hosted Zones

An R53 Hosted Zone is a DNS DB for a particular domain e.g. animals4life.org
- Hosts the actual records

Route53 is a global service

Very resilient - can handle failure of a whole region

R53 has relationships with various registries, e.g. .com, .net, .io

When we register a domain simonpeak.org
- Zone file is created
- Puts zone file onto 4x managed name servers
- Then communicates with .org registry
- Since R53 is authoritative, the records are added to the .org zone

Public and Private hosted zones

Every time we create a **hosted zone** R53 creates 4 name servers to host that domain
#### Record Types

ns records
- Allow delegation to occur through out DNS

A and AAAA:
- Host to IP address mapping
- AAAA record is for an IPV6 address

CNAME
- stands for Canonical Zone:
- equivalent of DNS shortcuts
- Points to another name record NOT an IP address
  - Limitation of CNAME you can't use it at the naked domain

ALIAS
- AWS specific record, not a DNS standard
- We use ALIAS record to solve limitation with naked domain
- ALIAS preferred over CNAMEs in AWS as requests are free
- Need to use aliases for certain services, e.g. ELB, CloudFront S3

MX record
- Email
- Links to an host or A record
- Could be multiple destinations, these are distinguished by Priority
  - Diff priority, always picks lowest unless there's a problem
  - Same priority could go to either

TXT record
- Add arbitrary text to a domain
- Use case: Prove domain ownership by adding some string
#### TTL values

How long should your DNS change take to propagate through other DNS servers?

#### DNS within a VPC

Choose whether DNS is enabled in VPC

Associate the hosted zone (public or private) with the VPC

If it is... the Subnet +2 address becomes the DNS lookup IP address

This can access the 4x public hosted DNS servers in the Public R53 Hosted Zone

==Exam: Enabling DNS within VPC enables a local DNS IP that can talk to any public dns zone==

Public Hosted Zone
- Available to the entire Internet

Private Hosted Zone
- Only available within associated VPCs

Split View
- Some stuff in private zones, some in public for the same domain name
- e.g. accounting related DNS records

#### Routing Types
##### Simple Routing

> tldr; use it when routing requests to one service

Each record can have multiple target IP address

No health checks. Just returns a bunch of IP addresses ,client then randomly picks one.

##### Health Checks

> tldr; worldwide checks from aws regions separate from R53 but are used by records

Can check anything over the public internet

Types of health checks:
- TCP/HTTP(S)
- HTTP(S)
- String Matching (within the response, e.g. OK)

Based on health checks, endpoint is either HEALTHY or UNHEALTHY
- Endpoint
- CloudWatch Alarm check
- Calculated Checks (checking lots of individual components in an application)

If the check fails, can send notification to SNS topic.

If 18% of health checkers report as healthy then the check is healthy

##### Failover Routing

> tldr; Used for active / passive routing

If primary target is unhealthy then the secondary record is returned and traffic goes there

##### Multi Value Routing

> tldr; mixture between simple and failover, if you have a set of targets and want one returned at random

Many records with the same name, e.g.

Returns up to 8 records to client

www -> A -> 1.2.3.3

www -> A -> 1.2.3.4

www -> A -> 1.2.3.5

Not a substitute for a load balancer - doesn't handle the actual network connection

##### Weighted Routing

> tldr; Use a priority to determine how often a particular record is returned

www -> 40 -> A -> 1.2.3.3

www -> 40 -> A -> 1.2.3.4

www -> 20 -> A -> 1.2.3.5

Can be used along with health-checks

- If record is select but it's unhealthy then another record is returned instead

Lets us send X amount of traffic to a certain host, e.g. testing a new feature on one host

##### Latency Based Routing

www -> us-east-1 -> A -> 1.2.3.3

www -> us-west-2 -> A -> 1.2.3.4

www -> ap-southeast-2 -> A -> 1.2.3.5

AWS uses an IP address lookup to determine where use is based, e.g. Australia

AWS maintains a list of latency between users's general location and AWS regions

Note: List that AWS maintains isn't real time, doesn't account for networking issues

##### Geolocation Routing

www -> state/country/continent/default-> A -> 1.2.3.3

www -> state/country/continent/default-> A -> 1.2.3.4

www -> state/country/continent/default-> A -> 1.2.3.5

If user is in US, which state are they in?

If not in US, which country are they in?

Which continent?

If nothing matches return default (catch all is optional)

Either returns the best match or default

Can be used to limit access to US only

==Exam: Not about proximity, it's about matches to rules==

##### Geoproximity Routing

> tldr; aims to calculate distance between customer and record and answer with lowest distance

Needs location of resources

And locations of customers

And a bias - to influence which resource is chosen

Then we can create rules

#### Route 53 Interoperability

> tldr; There are two seperate parts - registering and hosting a domain.

You can register a domain with Hover and host the DNS with R53

More common to see R53 for domain hosting - that's where the sweet, sweet functionality comes from

#### DNSSEC on Route53

Depends on KMS KSK keys

Keys need to be in us-east-1 region

Route53 actually handles the key management after initial stage

Delegator signer record
- Handled by R53 if it's the domain registrar
	- Changes are passed on to 
- Otherwise we have to handle it manually

We also need some Cloudwatch alarms
- To handle problems

#### PrivateLink

> tldr; access someone else's resources securely via a vpc endpoint such as a marketplace appliance

- Need multiple endpoints for HA
- Doesn't support ipv6
	- Supports private DNS

Direct Connect Site to Site VPN and Peering are also supported

Usually involves NLB to provide HA with provider using cross-zone LB

This is provided in our VPCs via an endpoint, can be protected by SG and NACLs
### Endpoints

#### Gateway Endpoints

> tldr; private access to s3 and dynamodb from a vpc

You can access those services even from private subnets.

==Exam: Gateway Endpoint does not go into a subnet==

==Exam: Regional and HA across the region, don't need to worry which AZ its placed it==

Added to route table as a prefix list with target of gateway endpoint

We can also add a policy allowing the GW End point to access only certain S3 buckets

Can only access resources in the same region

==Exam: Can only access endpoint from the VPC they are associated with==

Also we can keep our S3 bucket private (with execption for GWE, the rest explicit deny) -- win for security

#### Interface Endpoints

> tldr; another way of providing access to public services from private subnets
> tldr; you can access more services than gw endpoints

Interface Endpoints don't use routing, they use DNS.

==Exam: Not highly available by default!==

So we need an Interface Endpoint in each subnet in each AZ in a VPC.

Access controlled with Security Groups - can't do this with GW Endpoints.

Can still use policies like GW Endpoints.

Only support TCP and IPv4.

Behind the scenes, interface endpoints use PrivateLink (external services injected into VPC)

Interface Endpoints primarily use DNS (instead of prefix-list on route tables used by GWE)

Interface Endpoint provides new service DNS, e.g.

vpce-123098-xyz.sns.us-east-1.vpce.amazonaws.com

We actually get multiple DNS options:
+ Regional DNS
+ Zonal DNS

Application can use these DNs entries or...

PrivateDNS, which is a private DNS Hosted Zone in R53 which means application's don't need to their config updated.

PrivateDNS great when we cant modify application's config but still need to hit AWS Public Services via a private subnet.

#### Endpoint Policies

We have a policy on the endpoint that determines who can access the endpoint.

Only limits access if the service is being accessed through the endpoint.

Request -> Endpoint -> Policy -> S3 (seperate permissions required)

How do we restrict bucket access to a particular endpoint? 

Bucket policy example:

Condition: { "StringNotEquals": {aws:sourceVpce: "vpce-1238"}}

We'd usually use this to match endpoints.

Note: for external IPs, we can limit access using SourceIP, e.g. 

"Condition": { "NotIpAddress": { "aws:SourceIp": ["192.398.99.1/24"] }}

#### VPC DNS and DNS Endpoints

> tldr; VPC .2 address is reserved for DNS in a private subnet (called VPC +2 address)

E.g. in 10.16.0.0/16 the VPC DNS address would be 10.16.0.2

In every subnet the .2 address is also reserved. (? check this)

Called the Route53 resolver -- not accessible outside the VPC.

R53 Resolver provides public hosted zones and private hosted zones.

What about resources in a VPC that want to access corp network?
- R53 resolver doesn't know about corp network, only AWS Public/Private Zones and Public internet.
- Historically an EC2 based forwarder was used to address this.
- The corp network can also forward queries to the resolver that don't resolve internally.
#### Route53 Endpoints 
> tldr; Support DNS lookups in and out of a VPC

VPC Interfaces (ENI) Delivered over VPN or DX
+ Highly available and scale as needed.
##### Inbound and Outbound
+ Inbound acts like the EC2 instance forwarder example above, on-prem DNS can forward requests to it
+ Outbound is a conditional forwarder for R53 to on-prem
	+ Rules to determine what's forwarded, e.g. everything in corp.a4l.org
	+ Outbound endpoints have unique IP addresses, so can be whitelisted on FW

![[Images/r53_endpoints.png]]
More info:
https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html

#### IPV6 in a VPC

> tldr; no worries about having enough IP addresses, IPV6 is basically unlimited so no diff between public/private ip address allocation

We use Egress-only IGWs for outbound internet IPV6 (they are free, unlike NAT GWs)

When we enable IPV6 in a VPC we can be allocated a range from AWS or provide our own.

We can have both an IGW and an Egress Only IGW in the same VPC.

Note IPV6 traffic can go through an IGW (to enable inbound and outbound traffic)

However NAT GW does not support IPV6.

![[Images/ipv6_vpc_structure.png]]

We can migrate an IPv4 VPC to IPv6, don't need to create a new VPC.

Note: PrivateLink and Interface Gateways don't support IPv6 to the same degree (some limitations)

#### Designing VPCs for HA

> tldr; We need to calculate this based on Available AZs and cost

Think about the minimum number of servers the appliction needs. 

Take an application that requires 6 EC2 instances.

e.g. If you split 6 EC2 instances across 2 AZs to "add HA" but the minimum required for functioning is 6 you still have a downtime if we lose an az.

6 in each AZ isn't cost effective. (paying for 2x instances but you do get HA)

However 3x3 we only need an extra 3 instances (50% over-provisioning, more cost effective)

**Process to follow to work this out:**

Q. How many AZ failures can be tolerated -- this is the "AZ buffer"?
Q. How many AZs does the region we're going in actually have?

Let's say we have 6 AZs. Minus 1 buffer AZ = 5 Nominal AZs.

Minimum app requirement: 5 AZs

Nominal Instances - Nominal AZs = 1 instance per AZ.

(6 instances instead of 5 -- this is only 20% over provisioning)

==Exam: Might ask for minimum number of AZs or the most cost effective required.==

##### Tiers

> tldr; a 3 tier web app - Presentation -> Logic -> Data, outdated in AWS

1x subnet per AZ

Traditionally this was done with isolated subnets.

Multi-subnet approach is no longer good in AWS.

Note: 1 Subnet - No HA and No ability to use NACLs (they apply at subnet boundary)

Do we need another tier for DBs in AWS?

Not necessarily because public addressing isn't automatically applied in public subnet.

AWS also provide security groups per ENI or resource. We can disallow acess from everywhere except the EC2 instances.

So why would we need a tier? If it was required in the solution design.

Also if we need NACLs - if DBs are in different tier / subnets then we can apply NACLs on the subnet boundary.

Another reason: Route tables are associated with subnets

General rule with routing - whatever you configure for a subnet applies to everything *in- that subnet.

If web and DB server are in the same subnet, this can be a problem.

Splitting subnets gives more flexibility.

Gateways often need this... NAT GW needs to be in public subnet.

How do we determine the number of tiers we need for our application?

1) Ignore HA to start with
2) How many subnets does our app need?
	1) You don't need multiple subnets to control public and private addressing
	2) Or security
	3) All of these can be mixed and matched in one subnet
	4) Except if we want to filter via NACL
3) Different routing decisions = multiple subnets
4) Load Balancers
	1) note an internet facing ALB can communicate with private instances
5) Take the number you arrived at and multiply by HA requirements
	1) Minimum number of AZs we need to deliver HA reqs

Keep in mind: Public subnet can have both public and private instances
Keep in mind: Private subnet can only have private IPs
Keep in mind: Don't need additional subnet for Load Balancer
Keep in mind: More subnets per az needed if we need different routing, if you need certain security, or if the requirements dictate some stuff can't be in same subnet.

