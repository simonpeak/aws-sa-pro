## Compute Scaling & Load Balancing

> [!info] Demanding, highly available workloads in AWS
> 
### Regional and Global AWS Architecture

> tldr; global and regional perspectives on applications

Global and Regional - two types of application

Route 53 DNS can direct customers to a certain region

Health checks are key - where do we send the traffic?

CDNs used for performance

Cloud Front -> Web Tier -> Compute Tier -> Storage Tier -> Caching Tier -> DB TIer -> App Services (e.g. Firehose)

### EC2 Purchase Options

> tldr; Launch Types = Purchase Options

On-Demand
- The default - use this unless you need something else
- Your instance is spun up on an EC2 Host with other customers (isolated)
- Per second billing
  - Not charged if instance is off
  - But still charged for storage
- No priority access to remaining capacity if there's a major outage
- Ideal for short-term workloads

Spot

- Cheapest way to get access to EC2 capacity
- Up to 90% discount
- Spare capacity on EC2 host sold at discounted rate
- Can be terminated at any time
  - Application must be able to handle interruption
- You set the maximum price you're willing to pay, but you only ever pay the current spot price
- If there's a shortage of capacity, your price might jump up
  - If max price is below the new price, your instance will get terminated
- Suit flexible workloads where timing isn't so important

Reserved

- Reserved is for long term consistent usage of EC2
- Effect of reservation

  - Reduce or no per/second cost
- It's possible to purchase reservation and not use it

  - You'll still be billed
- Can be locked to AZ - does reserve capacity
- Can be locked to a region - doesn't reserve capacity
- Can have a partial effect

  - If you've paid for a T3 instance and go for a larger one you'll still get some discount
- Standard Reserved

  - 1 year or 3 year terms (more discount for longer)
- Scheduled Reserved

  - Frequency duration and time specified upfront
  - You might only need 5 hours a day for batch capacity
- Payment methods: 

  - Per second fee over the reservation period
  - Upfront gives more discount
  - Partial-upfront a combination

Dedicated Host

- You're paying for the actual physical host
- Instances running on the host have no extra charge
- If the hosts run out of capacity, you won't be able to launch any more instances
- Use when: Licensing is based on physical machines (socket/core)
- Host affinity - ensures the same instance starts on the same host

Dedicated Instance

- Middle ground between Dedicated Host and default
- Your instances are the only thing running on a host
- Once you launch an instance AWS chuck it on a host and don't put other customer's instances on there
- 1 off hourly fee to use this feature
- Fee for dedicated instance itself
- Use for industries where you cannot share underlying hardware with other customers

Priority of capacity reservation

- Certain events can result in not enough capacity in a region or AZ
- AWS has a priority order
  - First reserved instances
  - Then on-demand
  - Then spot
- Remember Region Reservations don't reserve capacity so are not prioritised
- AZ specific 'Zonal Reservations' do reserve capacity
  - If you launch into a diff region you don't have capacity reservation or discount
- On-demand capacity reservation
  - You pay for a reservation so you have reserved capacity

Savings Plans

- Hourly Comitment for a 1 or 3 year term
- General computer amount
  - Valid for various compute services
  - Quite flexible if evaluating different services
- Or specific EC2 savings plan
  - Just for EC2
- If you have a savings plan, you'll get a discount until you've consumed your commitment

### EC2 Networking

> tldr; an ec2 instance can have multiple network interfaces, with different SGs

You can add additional ENIs E.g. Primary ENI, Secondary ENI

==Exam: Different Subnet is fine, different AZ not supported for the 2nd ENI==

Security Groups are associated with ENI not EC2 instance

Each ENI can be protected by a NACL around its subnet

==Exam: ENIs are what's important, not the instances - security groups go on ENIs==

==Exam: Since every instance has a ENI, and is allocated 1 private IP address, we know each EC2 instances have an IP that never changes. The private IP is assigned to the ENI which never changes.==

Secondary IPs addresses might be MGMT addresses

- These IP addresses are exposed to the OS
- IP addresses can be moved to a different instance by moving the ENI

A public address can also be assigned to an instance

- This is allocated to the primary ENI
- This is not static / fixed
- The instance never has this in its config - IGW does the routing

Want a static IP address? Use an Elastic IP address

- Allocated to the AWS account
- Then associated with the instance OR ENI
  - Associating with instance = associating with Primary ENI
  - If done, existing IPv4 is gone and we can't get it back
  - If we remove the elastic ip, instance gets a new public ipv4 address
- Elastic IPs are usually used for things that can't use DNS / require static IPs

==Exam: Elastic IPs get charged when not in use but not when allocated!==

Each ENI has one or more security groups

Source/Destination check - Per ENI config option enabled by default

- Drops traffic where ENI isn't the source or dest address

- We'd need to turn this off when creating a NAT instance

Why secondary interface?

- Seperate management traffic from other traffic
- Software licensing that requires a specific mac address -> attach this to eni then can move it round to other instances
- Security/Network appliances - traffic going in one interface and out another
- Multi-homed instances with different workloads/roles on specific subnets
- Low budget HA setup

### Bootstrapping vs AMI baking

> tldr; do we use a userdata script or build an image?

Hypothetical:

We have a ASG + an LB and have a scaling event

- This adds 1 EC2 instance
- This can take 5 mins to launch the instance, then even longer to install, config and test application
- What happens if this takes too long? 
- By the time all this happens it might be too late, users pissed off etc

With user-data script:

- Base OS image + Dependencies + Updates + App Install + App config (fast)
- Flexible but slow

With AMI image pre-baked:

- Slow stuff already done ahead of time

We want to do the heavy lifting ahead of time, and leave the final config (which is quick)

Bootstrapping and AMI baking are not mutually exclusive

We can launch an EC2 instance off a custom AMI, then run a script

### Elastic Load Balancer Architecture (ELB)

> tldr; job of load balancer to accept connections and send to appropriate compute; abstraction of backend servers, loose coupling, allows scaling

- When setting up we choose which subnets the ELB goes in
- And whether it's IPv4 or Dual Stack
- **An ELB is actually multiple nodes set up in chosen subnets** plus DNS A to send traffic to these these nodes**
- Highly available - if a node fails it's replaced
- We need to decide internet facing or internal
  - **Internet facing get public IPv4 addresses, internal ones don't**
- Load Balancers are configured with listeners
  - Which port and protocol are accepted?
  - Where does the traffic get sent?
- **Internet facing LB can route to public and private addresses**
  - The public part is about the LB being accessible over the internet
- Load balancers need at least **8+  IP addresses per subnet**
  - In practice this means /28 subnet - 16 ip addresses - 5 reserved = 11
  - **AWS recommend /27 or larger so that it can scale (preferred answer generally to allow space for nodes too)**

#### Cross Zone Load-Balancing

> tldr; Cross Zone LB can send the traffic to instances in a different AZ

Historic limitation where each LB node in an AZ would distribute traffic unevenly, when you had say 1 node in AZ1 and 5 in AZ2

Cross Zone LB enabled by default now - didn't used to be

### Session State

> tldr; per user config for an application, e.g. shopping cart, user login info

Disruption to session state results in bad UX

LB needs to maintain the state so we can recover from failover without bad UX

Externally Hosted Session - results in instances that are stateless

### ELB Evolution

> tldr; 3 types of LB available, avoid V1 LBs in favour of newer V2

CLB - Classic LB (v1)

- Not really Layer 7, lacks features, only 1 SSL per CLB

==Exam: Don't use CLB==

ALB - Application Load balancer (v2)

- True Layer 7 device
- Use for HTTP/s/WebSocket

NLB - Network Load Balancer (v2)

- Use these for non http(s) traffic
- Such as email servers

Main advantages of V2: Faster, cheaper, support target groups, rules
### Which Load Balancer?

> tldr; classic vs application vs network

**Classic Load Balancer**
Avoid Classic LB - doesn't support SNI which means 1 SSL per CLB

*SNI, or Server Name Indication, is an addition to the TLS encryption protocol that enables a client device to specify the domain name it is trying to reach in the first step of the TLS handshake, preventing common name mismatch errors.*

**Application Load Balancer**
Layer 7 - HTTP or HTTPS only
No SSH, or anything else
No TCP/UDP
Can inspect all L7 info
- Cookies
- Custom headers
- User location
- App behaviour
HTTP / HTTPs are always terminated on the ALB - no unbroken SSL (security concern)
No end-to-end encryption
- Also means ALBs need SSL certs
ALBs are slower than NLB (more levels of network stack to process)
But they can evaluate application health at L7
- Can make an application layer request to make sure things are tip-top

Rules:
- What to do with traffic that arrives at a listener?
	- Default rule = catchall
	- Rule conditions: host-header, http-header, http-request-method, path-pattern, query-string & source-ip
	- Actions: Forward, redirect, fixed-response, authenticate-oidc, authenticate-cognito
	- In action: Send end users to different target groups if they come from a different IP address (source-ip)
	- The key: routing decisions based on anything you can see at L7

==Exam: If you need end-to-end TLS you need to use NLB==

**Network Load Balancer**
Layer 4 - TCP, TLS, UDP, TCP_UDP
No visibility of HTTP or HTTPS - no session stickiness
Way faster than ALBs - no computation heavy L7 processing
Just checks ICMP/TCP handshake, not app aware
NLBs can have static IPs - useful for whitelisting
Does handle end-to-end TLS/SSL (NLB isn't are of what the traffic is so can just foward it)
==Exam: NLBs are used for PrivateLink to provide services to other VPCs==
==Exam: Default to NLB if need to load balance non HTTP/HTTPs traffic==
==Exam: Use NLBs for the following otherwise ALB==
- Unbroken encryption
- Static IP for white listing
- Absolute best performance
- Protocols that aren't HTTP or HTTPs
- Privatelink

### Connection Stickiness

> tldr; ALB can store session info, it's part of the target group config

Client generates a cookie which locks device to a single backend for a duration

Session moves when:
- If an instance fails, client goes to new instance and stays there
- Cookie expires

Problems: Since users are locked, heavy user might saturate a server

**Optimal solution: Externally hosted session state, but ALB is ok if application doesn't or can't support it**

Cookie name: AWSALB

==Exam: Look out for logout, lost carts, lost progress -- these indicate lost session state==

### Auto Scaling Groups

> tldr; automatic scaling and self healing

Key thing is the desired number of instances

ASG performs the necessary scaling actions on our behalf.

They work across AZs

Simple EC2 HA
- Create a launch template
	- Automatically build an instance
- Set auto scaling group to use multiple subnets
- Set min/max number of instances to 1
- The instance will recover if shutdown, even if in different AZ

ASG + Load Balancers
- When load goes up, new instances go into the Target Group and can receive traffic
- We can use ALB health checks to scale in /out
- Be careful of health check, it need to cover end to end use of application
- But make sure your DB having a problem doesn't lead to all your EC2 instances getting re-created

Scaling Processes
- Launch and Terminate
- Both can be suspended
- If Launch is suspended, no new instances launched
- Same for Terminate
- AddToLoadBalancer - add to LB on launch?
- AlarmNotification - does instance respond to CW Alarms?
- HealthCheck - Are instance health checks on or off?
- ReplaceUnhealthy - yes or no?
- Scheduled Actions - Does the ASG perform any scheduled actions?
- You can also set an instance to InService or Standby
	- If set to stand by you can perform maintenance on an instance without triggering the ASG
- To prevent rapid scaling use cool downs
- Think about smaller instances rather than large ones - allows more granularity

ASGs are free
ASG controls the WHEN and the WHERE
Launch Templates: Define WHAT

#### ASG Lifecycle Hooks

> tldr; custom actions when ASG does stuff

Instances wait until timeout then either CONTINUE or ABANDON

Or you resume the ASG process via CompleteLifecycleAction

Example of custom actions:
- Index or load data

Same can be done with instance Termination Hook

Example:
- Backup data or logs

Can integrated with SNS or Event Bridge

#### ASG Scaling Policies 

Scaling polices update the 'desired number'

Manual scaling
- we just increase the number ourselves
Scheduled scaling
- Time based adjustment - e.g. during sales
Dynamic scaling
- Simple: e.g. CPU above 50%, add one instance
	- Can use lots of criteria
	- Needs CW alarms to trigger
- Stepped scaling
	- Add one instance if CPU is above 50% but add 3 instances if above 80%
	- These required CW alarms to trigger
	- AWS recommend this over Simple*
- Target Tracking
	- Say you want 40% aggregate CPU
	- Scale number of instance to achieve this
- SQS queue based scaling
	- If the queue is backing up let's add some more instances
- Cooldown period
	- Reduce flip-flipping

Why- is Step recommended over Simple?
- Simple is very blunt
- You end up adding or removing the same no. instances even with extreme moves
- Step allows rules for if the jump is big, e.g. going from 10% to 50% would add more instances than between 10% and 11%

#### ASG Health Checks

> tldr; can use EC2, ELB and Custom

EC2: Stopping, Stopped, Terminated, Shutting Down, Impaired (not 2/2 status) = unhealthy

ELB: Healthy = Running and passing ELB health check

Custom: External system

**Health check grace period:* Time allows before an instance is 'health checked'. Say you have a slow user data script, your instance might fail before its even ready without this.

### Connection Draining / De-registration Delay

> tldr; what happens when instances are being de-registered? users get cut off

#### Connection Draining 
Gracefully removing connections from an instance

CLB only - it's an old feature defined on LB itself (CLBs don't use target group architecture)

#### Deregistration Delay
The same thing on modern load balancers (ALB, NLB, GWLBs with subtle differences)

Defined on Target Group not LB

Stops sending new requests to targets, existing ones can continue until they complete naturally or deregistration delay is reached

Default = 300s

### X-Forwarded-For & Proxy Protocol
> tldr; When clients go through a LB we no longer see their IP address
#### X-Forwarded-For

HTTP and HTTPs (Layer 7)
It's a header that keeps track of the original source
Keeps track of other proxies on the way too
Needs to be supported by backend server
Only supported by ALB, (remember L7) not NLB
#### Proxy Protocol

Layer 4 version - supported by NLB
Added as a TCP header
Can work with a range of protocols including HTTPs and HTTP
Two versions: v1 (CLB) and v2 (NLB)
Not supported by ALB
If you need unbroken HTTPs, you'll want to use NLB + Proxy Protocol

#### Placement Groups

> tldr; influence where ec2 instances are placed on physical hardware

Cluster
- One AZ only
- Pack instances close together, tend to be on single EC2 host
- Best practice to create all at once
- Best practice to use same type of instance
- 10GB/s single stream vs 5GB/s normally
- Low latency
- Use Enhanced Networking
- Use case: High performance applications
Spread
- Keep instances separated
- Each instance is in a different rack
- Limit of 7 instances per AZ
- Provides infrastructure isolation
- Mitigates risk of a rack failure taking out multiple instances
- Not supported for Dedicated Instances or Hosts
- Use case: Small number of critical instances that need to stay up
Partition
- Like spread but you have more than 7 instances
- More instances than spread, you have to administer the location (ec2 can do it too)
- Can be created across multiple AZs
- More control over where instances go
- Use case: Huge scale parallel processing
- Use case: Topology aware applications
	- hfds
	- casandra
	- hbase
- Say you had a large multi instance FS, 75 instances, you want to have 25 in one place, 25 in another and the last 25 in a 3rd place

### Gateway Load Balancers

> tldr; AWS feature to help scale 3rd party security appliances

Could be a Firewall, packet analysis tool

Key components:
- GWLB Endpoints, kind of like interface endpoints but with improvements
	- Improvement: Can be added to route table as next hop
	- L3/4 device
	- Traffic enters and leaves VPC via these
- GWLB itself
	- Load balances packets across multiple instances
	- EC2 instances running security software

GENEVE protocol - encapsulates packets and sends to appliances

Packets are not altered

Source -> GWLB -> Encapsulated by GENEVE -> Scanned -> Returned 
