## Databases

> [!tldr] Flavours of relational and non-relational databases
### Relational Database Service (RDS)

DB *Server* As A Service, not DBAAS
The DB subnet group defines which subnets and IP ranges the DB cluster can use in the VPC that you selected.
Best practice: One DB Subnet Group per Instance
Multi AZ = Synchronous Replication
Read Replica = Async
RDS backups go to an AZ managed S3 account -> backup is replicated to multiple AZs
#### Costs
1. Instance size & type
2. Multi AZ or not
3. Storage type & amount
4. Data transferred
5. Backups & Snapshots
	* You get the amount of storage for DB in Snapshot space for free
6. Licensing
#### High Availability
##### Multi-AZ instance
* Writes straight to EBS
* Synchronous replication
* Replication is at storage layer
* No access to standby even for reads
	* But backups occur from the standby
* Failover is done by DNS CNAME update
* ~60-120 sec failover time (longish)
* Only one secondary node, not 3!
##### Multi-AZ Cluster
* 1 Writer with 2 usable readers in different AZs
	* Can offload reads to the readers
* Writes considered committed when they are committed on 1 replica
* Writer endpoint and reader endpoint provided
* Each instance has its own local storage (unlike Aurora)
* Instance endpoints
	* Direct access to instance for fault finding, not for applications
* Storage is faster than Multi-AZ instance
	* Writes to super fast local storage first then flushed to EBS
* Replication is done with standard transaction logs
* ~35 second failover time

Some surprising limitations listed here: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html
#### Backup and Restore

Snapshots

* Don't happen automatically
* Work like EBS snapshots
* First snap is a full copy, rest are incremental
* Some IO impact, but occurs on standby in Multi AZ
* Snapshots don't expire, only go when deleted manually

Automated backups

* They work the same way as snapshots but are automated
* Data stored in S3 but we canâ€™t access it
* Every 5 mins, transaction logs are also taken
  * Allows for point in time recovery
* Retention max for these backups is 35 days
* Note: Backups still disappear after X days even if you delete the instance
  * Final Snapshot won't disappear though
* RDS allows snaps and backups to be replicated to another region
  * Not enabled by default

Restore

* Restores create a new instance

* You need to update applications to go to the new address

* Snapshots are restored to single point of time (time snapshot was taken)

* Snapshot restores take a long time! (measure this)

* Automated backups you can choose point in time (backup time, + log replay)

  

#### Read Replicas

> tldr; offload reads, also provided better RTO for instance failures

Read replicas aren't part of main DB instance in any way

They have their own endpoint address

Kept in sync using asynchronous replication

* Replica lag is a thing

(MultiAZ clusters replicas are syncronous)

Up to 5 per database instance

* Read replicas can have their own replicas too, but lag starts to compound

Read replicas can be in different regions

* Improved read performance in that region
* Can failover to this other region if the main region has a massive outage

Offer near 0 RPO, assuming no database corruption

Read Replicas are great for low RTO, but only for failures, don't work if data is corrupted



#### Security

* Encryption in transit
  * Can be enforced
* Encryption at rest is handled on the host using KMS key
  * Either AWS managed CMK (Customer Managed Key) or Customer Managed CMK
  * Can't remove encryption later on
  * DB engine doesn't actually know about encryption
    * Encryption applies to transaction/binary logs
* RDS MSSQL and Oracle support TDE
  * Done within the engine not instance storage
  * Oracle supports HSM
    * Engine encrypts data, RDS doesn't know about the keys
* IAM authentication
  * Policy maps IAM user to DB user
  * This can then be used instead of a password
  * User perms are still managed on the DB side
#### RDS Custom

> tldr; Niche topic, fills the gap between RDS and EC2 running a DB engine

Middle ground between RDS and still get access to SSH, RDP, Session Manager

Runs within your AWS account

* You see an EC2 instance, backups etc within your AWS account
* Standard RDS runs elsewhere and is injected into your VPC through and endpoint

You pause automation, add some custom stuff, then resume automation

Only works for Oracle and MS SQL Server
### Aurora

#### Aurora Provisioned

> tldr; very different from standard RDS, completely different architecture

Base unit is a Cluster

Multiple Endpoints:

* Read/Write Primary
* Reader Endpoint: load balance across all replicas
  * Much easier to manage read scaling, you don't need to worry about updating client connections
* Custom endpoints
  * You choose which instances are included

Compute wise:

* Single primary instance
* Plus 0 or more replicas
* Replicas can be used for reads during normal operations
* Provides read scaling and availability benefits

Storage wise:

* No local storage
* SSD based, no HDs possible
* 128TiB limit
* Billing based on high-watermark, maximum you've used
  * If you delete stuff storage costs won't go down
  * Recent versions of Aurora fix this
* Uses shared cluster volume
* Benefits: Faster provisioning, improved availability, better performance
* 6 replicas across multiple AZs
  * Synchronous replication
  * Replication happens at the storage level
  * Chances of losing data due to disk failure is much lower than standard RDS
    * Automatic repair with missing data from other cluster volumes

Read Replicas: Up to 15 replicas

* You can choose to fail over to any of these
* Failover is quicker
* Since storage is decoupled from compute, read replicas can be provisioned very quickly

Costs

* Compute = hourly charge per second, 10 minute minimum
* Better value than Standard RDS
* Storage GB/Month consumed, IO cost per request
  * New option: Predictable pricing
* 100% DB size in backups are included

Backup/snapshot restores still create a new cluster

Exciting features

* Backtrack - per cluster setting
  * Roll-back database to a previous point in time
* Fast clone
  * Copy on write, very quick way to spin up a copy 


#### Aurora Serverless

> tldr; An Aurora cluster with no need to provision the compute

ACU = Aurora Capacity Units

A cluster has Min & Max ACU

Can do to 0 and be paused, only charged for storage

Advantages:

* Scales
* Cost effective - only pay for what you use
* Consumption billing per second

How do ACUs work?

* AWS has a bunch of 'warm' ACUS

* ACUs are like nodes but get swapped in as required

* When they are swapped in, they get access to the storage
* More ACUs get added until you hit  Max ACU limit
* And removed as long is removed

How are connections handled given ACUs are swapped out?

* Proxy fleet
* Handles the connection, brokers a connection to the ACU
* Means all kinds of magic can happen without impacting clients

Use cases:

* Infrequently used applications
  * You only pay for DB resources when consumed
* New applications where size/capacity not known
  * DB will auto scale based on the load
* Variable workloads
  * If the load spikes you're covered
* Unpredictable workloads
  * Provision a cluster with a large number of ACUs
  * See how much you need
  * Better than overprovisioning a massive cluster on standard Aurora
* Development and test databases
  * Since it can pause during period of no load
* Multi-tenant applications
  * Where scaling is aligned between infrastructure size and incoming revenue

#### Aurora Multi-Master

> tldr; You know this already, it's like GR multi master

Standard Aurora: 1 R/W 0+ Read Only Replicas

Standard Aurora: Cluster endpoint, Read endpoint

In Multi-Master, all instances are running in RW mode

Multi-Master: No Cluster Endpoint anymore

Application can hit any instance

Works a bit like GR multi-master

Writes are proposed, Quorum agrees, write is comitted and replicated

These are also copied to each instance's instance cache

Benefits over single master:

* No interruption if master fails
  * Standard Aurora has to promote the other node
* Since all instances are read/write, application can go elsewhere
* **BUT: The application has to be designed to handle this**

### RDS Proxy

> tldr; Opening/closing connections has overhead

Proxy handles the connections
- Smoother failover
- Reduces connections on backend

Works well with Lambdas, which otherwise have to initialize a connection every time they are invoked

Since connections can be re-used, we don't need to open as many connections

When might we use it?
* Too many connection errors
* Small DB instances that get overloaded or have small max_connection limit
* AWS Lambdas, time saved per connection, can also use IAM Auth
* Long running connections (SAAS apps) - low latency
* Resilience to DB failure is a priority
  * Make it transparent to application
  * Instead of having to wait for failover, they are still connected to proxy

Key things for exam:
* Usable with RDS and Aurora
* HA by default
* Auto-scaling
* Connection pooling
  * Reduces constant openining/closing of connections
  * Multiplexing
* Only accessible within a VPC
  * Not public internet
* Accessed by a Proxy Endpoint
* Enforce SSL/TLS
* Can reduce failover time by 60% vs connecting to Aurora directly
* Abstracts failure away from application
### DynamoDB

> tldr; nosql public databases-as-a-service

Really more like "A database table as a service"

Key/Value & Document

No self-managed servers of infrastructure

* Reduces admin overhead

Public Service

Provisioned or on-demand (set and forget)

Highly Resilient

* Across AZs
* Or optionally global resilient

Data is replicated across multiple nodes by default

Point-in-time recovery

Event driven interaction

* If data in a table changes, trigger an event

#### **Creating a table**

* You choose a Partition key
  * Simple (Partition Key)
  * Composite is a combination of Partition and Sort Key
* Table contains Primary Key + Atrributes (optional)

#### **Capacity in Dynamo DB**

Capacity means speed, not storage space

Provisioned capacity

* You choose capacity value on a per table basis
* WCU and RCU
* **WCU = 1KB/s**
* **RCU = 4KB/s**

On-demand

* Unpredictable load
* Cost can be up to 5x as much as provisioned

You can switch between these with some limitations

Note: Each operation on a table consumes at least 1RCU or 1WCU (rounded up)

Also... burst pool! Lasts for 300 seconds.

* Be careful using this, you'll get errors if exceeded

#### **Backups**

* On demand backups - work like RDS snapshots
  * Manual process
  * They stick around until removed
  * Restore can be done in same or cross regsion, with or without indexes, and you can adjust encryption settings
* Point In Time Recovery
  * Not enabled by default
  * When enabled keeps continuous backups over a 35 day window
  * 1 second interval

Exam: NoSQL or Key Value mentioned - most likely DynamoDB

You don't get access to SQL Querying

Billed on RCU, WCU, Storage, Features -- no base cost

Reservations are available for a long term cheaper rate

#### **Query**

We can query based on Partition Key and optionally a Sort Key or range

**RCU is 4KB** so it's mostly more efficient to retrieve multiple items in a single query

Architectural benefit: Minimizing size of item as much as possible

Designed for brining back specific items, not searching

#### **Scan**

Steps through entire table

Less efficient but we can search for stuff

Expensive as involves reading many items

Even stuff that doesn't match and is 'discarded' counts as an RCU

#### **Consistency model**

Eventual Consistency or Strong Consistency

AZA (Storage Node) <- AZB (Leader Storage Node) -> AZC (Storage Node)

Usually takes milliseconds to propagate change

DDB has a fleet of entities which route connections to appropriate node

Writes are directed to leader node

* Writes are more expensive because there are fewer Leader nodes

Leader node then replicates to other AZs

But what if that hasn't happened yet?

* When we read from DDB we can choose Eventual Consistent or Strongly Consistent
* If we choose Eventual Consistent read
  * We are directed to one node at random
  * If that node hasn't been updated, we get stale data
  * Read cost = 50%

Eventual scales better and easier to implement

* Any node can have reads sent to it so cheaper

If your application needs up to date data, e.g. stocks medical, you will need Strong Consistency otherwise can save some money

#### **WCU Calculation**

If you store 10 items per second, at 2.5K average size per item

2.5 round up = 3WCU

10 per second * 3 WCU = 30WCU

#### **RCU Calculation**

10 items per second, 2.5K average size

2.5 fits in 4K so we have 1 RCU per read

10 per second 10 RCU Required for Strongly Consistent

For Eventually Consistent = 50% of Strong = 5 RCU

#### Indexes

> tldr; a different view of the data to help with query operation efficiency

#### **LSI: Local Secondary Index**

Alternative view of the data

* Different Sort Key, same Partition Key
* Share RCU and WCU capacities with base table
* You choose which attributes are projected into index
* Gets around the scan problem above
  * Only attributes that match our LSI are in the index
  * If there is no value, the record won't be included
  * Can reduce number of RCUs

Exam: LSIs must be created along with the base table

Exam: Max 5 LSIs per base table

#### **GSI: Global Secondary Index**

Very powerful, completely different perspective on the data

* Can be created at any time
* Different PK and SK
* GSIs have their own RCU and WCU allocations
* You can choose which attributes are projected into GSI
* Also Sparse - if no value for selected attribute, then they are not charged

Exam: 20 GSI per base table

Exam: Always eventually consistent

#### **Design Considerations**

* Be careful with projections
  * It takes capacity to project stuff into an index
  * But if you don't include the data, and query it later, you'll be doing extra scans then
* AWS recommendation: Use GSIs  (more flexible) unless you need need strong consistency
* Table design should reflect most common access parttern
  * Then less common ones are covered by indexes

#### Streams and Triggers

#### **Streams**

> tldr; an ordered list of item changes over a 24hour rolling window

> tldr; a bit like like MySQL triggers

Enabled on per table basis

Records Inserts, Updates, Deletes

Different view types affect what is recorded

* Keys_only (pk/sk)
* New_image (state after change)
* Old_image (state before change)
* New_and_old_images (before and after)

#### **Triggers**

> tldr; when changes occur, a lambda can be triggered

e.g. use it for messaging -> push notifications

Exam: Cost effective and serverless implementation of a trigger architecture

#### DynamoDB Accelerator (DAX)

> tldr; a cache product that is part of dynamo db

When using DAX, we need software installed on application instance. 

* Called the DAX SDK
* Transparent to the application
* App query goes to DAX -> DynamoDB

DAX operates within a VPC

* Needs to be deployed across AZs
* Cluster Service
  * Primary Node (RW)
  * Replica Node
  * Election system, managed by DAX

DAX maintains 2 differerent caches

* Item cache
  * Single items based on PK/SK
* Query cache
  * Stores collections of items retrieved through query or scan operations

Query response times: microseconds for cache hits

Milliseconds for cache misses

Cost savings: We're reducing the number of RCUs/WCUs used

Scaling: Up (bigger instances), Out (more instances)

DAX does handle write-through -> Persists data to DDB and to cache at same time

DAX not a public service (it's in a VPC)

Main benefit: Scaling, Cost reduction, Epic response times

Downside: Eventual Consistency only, not so much benefit for write heavy apps

#### Global Tables

> tldr; multi master, cross region replication concept for tables

You create tables in multiple regions, configure global table, then write to any of them

**Last writer wins used for conflict resolution**

Strongly Consistent reads available in same region you just wrote to

Otherwise eventual consistency

#### TTL

> tldr; A way to expire unneeded data from a Dynamo DB table

You specify a date and a time for an item when it's no longer required

Epoch based: Number of seconds since Epoc 1970 value

This sets up a clean up process

If item is older than TTL then it will be set to expire (can still be query)

Another automated process actually cleans up expired items

No performance overhead or charge for these

Can configure a stream on TTL deletion -> e.g. table auditing

* gives a 24 hour rolling window of the deletes

### ElasticSearch (ES)

> tldr: managed elastic, elk stack

Not serverless: Runs in a VPC using EC2 instances

Alternative to CWLogs or CW, but only if you have a specific need

Kibana = visualisation

Logstash = similar to CWLogs - requires logstash agent

Exam: Only use it over CW or CWLogs if there's a good region

### Athena

> tldr; a way to do adhoc queries on stuff stored in S3

No base monthly cost, pay for data consumed

Schema-on-read: Table-like translation

**Exam: Data on S3 is never changed, the schema modifies it in flight into a table like structure**

No need for ETL

You decide how you want the data to look, it's transformed on run

Many formats:

* xml
* json
* csv
* avro
* parquet
* orc
* apache
* cloudtrail
* vpc flow logs

Output can be sent to visualization tools

**Exam: There's no infrastructure, this is a serverless product**

**Exam: Preferred solution for querying AWS logs**

Athena Federated Query: Can query non-s3 data sources via connector

### Neptune

> tldr; Graph DB, relationships are as important as data

Exam: Like RDS for Graph based databases

Graph database:

* Nodes are things, e.g. Person, House, Credit Card
  * Can have attributes or tags
* Relationships between Nodes
  * e.g. Bob (node) -> Has a (relationship) -> Credit card (node)

Exam: If you see graph database mentioned it's probably Neptune

Use cases:

* Ideal for social media (fluid relationships)

* Fraud prevention

* Recommendation engines
* Network and IT operations
* Biology and other life sciences

### Quantum Ledger DB (QLDB)

> tldr; Immutable append only ledger based database

Cryptographically verifiable transaction log

You can always see a full history of changes

Good fit for: Banking, Finance, Medical where legitimacy and history of data matters

Serverless

3AZ resilience and replication within each AZ

Export options:

* Can stream data into Amazon Kinesis

* Batch export to S3

Document DB model

QLDB model:

* Current state
* Verifiable History of all past data states
* Each state is linked to previous state via a hash

Use cases:

* Finance - account balances and transactions
* Medical - full history of data changes matters, e.g. drug interactions
* Logistics - track movement of objects
* Legal - track usage and change of data (custody)


